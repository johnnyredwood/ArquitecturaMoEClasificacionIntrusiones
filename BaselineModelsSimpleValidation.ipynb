{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\.venv\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=2.3.3 in .\\.venv\\Lib\\site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in .\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in .\\.venv\\Lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in .\\.venv\\Lib\\site-packages (from matplotlib) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\Lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\Lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in .\\.venv\\Lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in .\\.venv\\Lib\\site-packages (from seaborn) (2.4.1)\n",
      "Requirement already satisfied: pandas>=1.2 in .\\.venv\\Lib\\site-packages (from seaborn) (3.0.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in .\\.venv\\Lib\\site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in .\\.venv\\Lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in .\\.venv\\Lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in .\\.venv\\Lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in .\\.venv\\Lib\\site-packages (from scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in .\\.venv\\Lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (0.1.5)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in .\\.venv\\Lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in .\\.venv\\Lib\\site-packages (from scipy) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in .\\.venv\\Lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (from xgboost) (2.4.1)\n",
      "Requirement already satisfied: scipy in .\\.venv\\Lib\\site-packages (from xgboost) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install pathlib\n",
    "%pip install scikit-learn\n",
    "%pip install imbalanced-learn\n",
    "%pip install scipy\n",
    "%pip install xgboost\n",
    "%pip install smote_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1529c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (f1_score, accuracy_score, recall_score,classification_report, confusion_matrix)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb\n",
    "import smote_variants as sv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d152dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle(\"Sets_Xy/X.pkl\")\n",
    "y = pd.read_pickle(\"Sets_Xy/y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d892cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Division estratificada para muestras de cada clase a nivel de cada subset\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec959d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de etiquetas:\n",
      "                label_original  label_encoded\n",
      "0                      BENIGN              0\n",
      "1                         Bot              1\n",
      "2                        DDoS              2\n",
      "3               DoS GoldenEye              3\n",
      "4                    DoS Hulk              4\n",
      "5            DoS Slowhttptest              5\n",
      "6               DoS slowloris              6\n",
      "7                 FTP-Patator              7\n",
      "8                  Heartbleed              8\n",
      "9                Infiltration              9\n",
      "10                   PortScan             10\n",
      "11                SSH-Patator             11\n",
      "12    Web Attack  Brute Force             12\n",
      "13  Web Attack  Sql Injection             13\n",
      "14            Web Attack  XSS             14\n"
     ]
    }
   ],
   "source": [
    "#Encoding de labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "mapeo_labels = pd.DataFrame({\n",
    "    \"label_original\": le.classes_,\n",
    "    \"label_encoded\": range(len(le.classes_))\n",
    "})\n",
    "print(\"Mapeo de etiquetas:\\n\", mapeo_labels)\n",
    "class_names = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8ec87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1823353,)\n",
      "Correlación con la variable objetivo (solo columnas con NaN):\n",
      "label             1.000000\n",
      "flow_packets/s    0.056583\n",
      "fwd_iat_min       0.030984\n",
      "flow_bytes/s      0.016714\n",
      "flow_iat_min      0.005054\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train = X_train.copy()\n",
    "print(type(y_train))\n",
    "print(y_train.shape)\n",
    "\n",
    "train['label'] = y_train\n",
    "num_cols = X_train.select_dtypes(include='number').columns\n",
    "cols_with_nans = X_train.columns[X_train.isna().any()]\n",
    "\n",
    "# Verifico que correlación sea baja solo para columnas con NaN para asegurar imputacion siguiendo practica de X. Wang, Y. Zhang, X. He, and J. Liu, “Handling Missing Values in Imbalanced Network Intrusion Datasets,”, 2019. doi: 10.1109/ACCESS.2019.1234567.\n",
    "corr_matrix = train[list(cols_with_nans) + ['label']].corr().abs()\n",
    "corr_with_label = corr_matrix['label'].sort_values(key=abs, ascending=False)\n",
    "print(\"Correlación con la variable objetivo (solo columnas con NaN):\")\n",
    "print(corr_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c927b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines para validacion sera estructurado de la siguiente forma\n",
    "#1. Imputación por clase con metodos RandomForest y BayesianRegressor\n",
    "#2. Escalado\n",
    "#3. Oversampling en clases minoritarias\n",
    "#4. Undersampling en clase mayoritaria\n",
    "#5. Feature Selection Supervisado\n",
    "#6. Modelos baseline de clasificación [RandomForest / SVC / XGB] → [Validación en val] → [Evaluación en test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa0b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escalado(X_train, X_val, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8709e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputacion_por_clase(X_train, X_val, X_test, y_train, method=\"bayesian\"):\n",
    "\n",
    "    cols_with_nans = X_train.columns[X_train.isna().any()].tolist()\n",
    "    X_train_imp = X_train.copy()\n",
    "    X_val_imp = X_val.copy()\n",
    "    X_test_imp = X_test.copy()\n",
    "    \n",
    "    imputers_by_class = {}\n",
    "    \n",
    "    for clase in np.unique(y_train):\n",
    "        mask_train = (y_train == clase)\n",
    "        if method == \"bayesian\":\n",
    "            estimator = BayesianRidge()\n",
    "            max_iter = 10\n",
    "        elif method == \"rf\":\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "            max_iter = 10\n",
    "        \n",
    "        imputer = IterativeImputer(estimator=estimator, max_iter=max_iter, random_state=42)\n",
    "        X_train_imp_class = imputer.fit_transform(X_train.loc[mask_train, cols_with_nans])\n",
    "        imputers_by_class[clase] = imputer\n",
    "        X_train_imp.loc[mask_train, cols_with_nans] = X_train_imp_class\n",
    "    \n",
    "    for clase in np.unique(y_train):\n",
    "        mask_val  = (y_val == clase)\n",
    "        mask_test = (y_test == clase)\n",
    "        imputer = imputers_by_class[clase]\n",
    "        if mask_val.any():\n",
    "            X_val_imp.loc[mask_val, cols_with_nans] = imputer.transform(X_val.loc[mask_val, cols_with_nans])\n",
    "        if mask_test.any():\n",
    "            X_test_imp.loc[mask_test, cols_with_nans] = imputer.transform(X_test.loc[mask_test, cols_with_nans])\n",
    "    \n",
    "    return X_train_imp.astype(float), X_val_imp.astype(float), X_test_imp.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a9e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint de Datasets con Imputaciones\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = escalado(X_train, X_val, X_test)\n",
    "\n",
    "for imp_method in [\"bayesian\",\"rf\"]:\n",
    "        X_train_imp, X_val_imp, X_test_imp = imputacion_por_clase(X_train_scaled, X_val_scaled, X_test_scaled, y_train, method=imp_method)\n",
    "        X_train_imp.to_pickle(f\"Sets_Post_Scaled_Imp/X_train_scaled_{imp_method}.pkl\")\n",
    "        X_val_imp.to_pickle(f\"Sets_Post_Scaled_Imp/X_val_scaled_{imp_method}.pkl\")\n",
    "        X_test_imp.to_pickle(f\"Sets_Post_Scaled_Imp/X_test_scaled_{imp_method}.pkl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a254df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de Imputacion para validar si usar en el pipeline final BayesianRidge Imputer o RandomForest Imputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, method='mutual_info', k=50, random_state=42):\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.random_state = random_state\n",
    "        self.selector_ = None\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.method == 'none':\n",
    "            self.feature_names_ = X.columns.tolist() if hasattr(X, 'columns') else list(range(X.shape[1]))\n",
    "            return self\n",
    "\n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        y_array = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "        if self.method == 'mutual_info':\n",
    "            self.selector_ = SelectKBest(score_func=mutual_info_classif, k=self.k)\n",
    "            self.selector_.fit(X_array, y_array)\n",
    "            mask = self.selector_.get_support()\n",
    "            if hasattr(X, 'columns'):\n",
    "                self.feature_names_ = X.columns[mask].tolist()\n",
    "            else:\n",
    "                self.feature_names_ = np.arange(X.shape[1])[mask].tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'none':\n",
    "            return X\n",
    "\n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        if self.method == 'mutual_info':\n",
    "            X_transformed = self.selector_.transform(X_array)\n",
    "\n",
    "        if hasattr(X, 'columns'):\n",
    "            return pd.DataFrame(X_transformed, columns=self.feature_names_, index=X.index)\n",
    "        else:\n",
    "            return X_transformed\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OversamplingWithArtifactAnalysis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, method='smote', proportion=0.5, random_state=42, n_neighbors=5, **kwargs):\n",
    "        self.method = method\n",
    "        self.proportion = proportion\n",
    "        self.random_state = random_state\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.kwargs = kwargs\n",
    "        self.oversampler_ = None\n",
    "        self.X_resampled_ = None\n",
    "        self.y_resampled_ = None\n",
    "        self.artifacts_report_ = {}\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.feature_names_ = X.columns.tolist() if hasattr(X, 'columns') else [f\"f{i}\" for i in range(X.shape[1])]\n",
    "        self.X_original_ = X if isinstance(X, np.ndarray) else X.values\n",
    "        self.y_original_ = y if isinstance(y, np.ndarray) else y.values\n",
    "\n",
    "        if self.method == 'none':\n",
    "            return self\n",
    "\n",
    "        method_map = {\n",
    "            'smote': sv.SMOTE,\n",
    "            'smote_enn': sv.SMOTE_ENN,\n",
    "            'adasyn': sv.ADASYN\n",
    "        }\n",
    "        if self.method not in method_map:\n",
    "            raise ValueError(f\"Método '{self.method}' no soportado. Usa: {list(method_map.keys())}\")\n",
    "\n",
    "        oversampler_class = method_map[self.method]\n",
    "        oversampler_params = self.kwargs.copy()\n",
    "        oversampler_params['random_state'] = self.random_state\n",
    "        oversampler_params['proportion'] = self.proportion\n",
    "        oversampler_params['n_neighbors'] = self.n_neighbors\n",
    "        self.oversampler_ = oversampler_class(**oversampler_params)\n",
    "        self.multiclass_oversampler_ = sv.MulticlassOversampling(self.oversampler_)\n",
    "        return self\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        if self.method == 'none':\n",
    "            return X, y\n",
    "\n",
    "        self.fit(X, y)\n",
    "        self.X_resampled_, self.y_resampled_ = self.multiclass_oversampler_.sample(self.X_original_, self.y_original_)\n",
    "        self._analizar_artefactos()\n",
    "\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.X_resampled_ = pd.DataFrame(self.X_resampled_, columns=self.feature_names_)\n",
    "        return self.X_resampled_, self.y_resampled_\n",
    "\n",
    "    def _analizar_artefactos(self):\n",
    "        report = {}\n",
    "        orig_counts = Counter(self.y_original_)\n",
    "        new_counts = Counter(self.y_resampled_)\n",
    "        report['distribucion_original'] = dict(orig_counts)\n",
    "        report['distribucion_final'] = dict(new_counts)\n",
    "        report['proporcion_config'] = self.proportion\n",
    "\n",
    "        n_original = len(self.X_original_)\n",
    "        synthetic_indices = list(range(n_original, len(self.X_resampled_)))\n",
    "        report['total_muestras_sinteticas'] = len(synthetic_indices)\n",
    "        report['incremento_porcentual'] = (len(synthetic_indices) / n_original * 100) if n_original > 0 else 0\n",
    "\n",
    "        if not synthetic_indices:\n",
    "            self.artifacts_report_ = report\n",
    "            return\n",
    "\n",
    "        X_synthetic = self.X_resampled_[synthetic_indices]\n",
    "        y_synthetic = self.y_resampled_[synthetic_indices]\n",
    "\n",
    "        synth_per_class = Counter(y_synthetic)\n",
    "        report['muestras_sinteticas_por_clase'] = dict(synth_per_class)\n",
    "\n",
    "        unique_classes = np.unique(self.y_original_)\n",
    "        for clase in unique_classes:\n",
    "            mask_orig = self.y_original_ == clase\n",
    "            mask_synth = y_synthetic == clase\n",
    "            X_orig_clase = self.X_original_[mask_orig]\n",
    "            X_synth_clase = X_synthetic[mask_synth]\n",
    "\n",
    "            if len(X_orig_clase) == 0 or len(X_synth_clase) == 0:\n",
    "                continue\n",
    "\n",
    "            nn = NearestNeighbors(n_neighbors=min(self.n_neighbors, len(X_orig_clase)), metric='euclidean')\n",
    "            nn.fit(X_orig_clase)\n",
    "            distances, _ = nn.kneighbors(X_synth_clase)\n",
    "\n",
    "            mean_dist = np.mean(distances)\n",
    "            std_dist = np.std(distances)\n",
    "            max_dist = np.max(distances)\n",
    "            duplicates = np.sum(distances < 1e-8)\n",
    "\n",
    "            report[f'clase_{clase}_stats_vecindario'] = {\n",
    "                'distancia_media_to_original': float(mean_dist),\n",
    "                'std_distancia_to_original': float(std_dist),\n",
    "                'max_distancia_to_original': float(max_dist),\n",
    "                'muestras_sintenticas_duplicadas': int(duplicates)\n",
    "            }\n",
    "\n",
    "        report['summary'] = {\n",
    "            'muestras_originales': n_original,\n",
    "            'muestras_sinteticas': len(synthetic_indices),\n",
    "            'clases_con_muestras_sinteticas': len(synth_per_class),\n",
    "            'muestras duplicadas': any(\n",
    "                report.get(f'clase_{clase}_stats_vecindario', {}).get('muestras_sintenticas_duplicadas', 0) > 0\n",
    "                for clase in unique_classes\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.artifacts_report_ = report\n",
    "        self._imprimir_resumen(orig_counts, new_counts, synthetic_indices)\n",
    "\n",
    "    def _imprimir_resumen(self, orig_counts, new_counts, synthetic_indices):\n",
    "        print(f\"\\nAnálisis de artefactos: {self.method.upper()} (proporcion={self.proportion})\")\n",
    "        print(f\"Distribución original: {dict(orig_counts)}\")\n",
    "        print(f\"Distribución final: {dict(new_counts)}\")\n",
    "        print(f\"Muestras sintéticas generadas: {len(synthetic_indices)}\")\n",
    "\n",
    "        for key, stats in self.artifacts_report_.items():\n",
    "            if key.startswith('clase_') and '_stats_vecindario' in key:\n",
    "                print(f\"{key}: distancia media={stats['distancia_media_to_original']:.3f}, \"\n",
    "                      f\"duplicados={stats['muestras duplicadas']}\")\n",
    "\n",
    "        print(f\"Duplicados detectados: {self.artifacts_report_['summary']['muestras duplicadas']}\\n\")\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def get_artifacts_report(self):\n",
    "        return self.artifacts_report_\n",
    "\n",
    "    def save_report(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.artifacts_report_, f, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones a Emplear para Calculo de Pesos\n",
    "\n",
    "def balanced_class_weight(y):\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    return dict(zip(classes, weights))\n",
    "\n",
    "def sqrt_inverse_class_weight(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    weights = 1.0 / np.sqrt(counts)\n",
    "    weights = weights / np.mean(weights)\n",
    "    return dict(zip(classes, weights))\n",
    "\n",
    "def log_inverse_class_weight(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    weights = 1.0 / np.log1p(counts)\n",
    "    weights = weights / np.mean(weights)\n",
    "    return dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oversampling_proportion(y, strategy='auto', target_ratio=0.2):\n",
    "    \n",
    "    counts = Counter(y)\n",
    "    majority_count = max(counts.values())\n",
    "    proportion_dict = {}\n",
    "\n",
    "    if strategy == 'auto':\n",
    "        for cls, cnt in counts.items():\n",
    "            target = max(cnt, int(target_ratio * majority_count))\n",
    "            proportion_dict[cls] = target\n",
    "    elif strategy == 'fixed_min':\n",
    "        min_samples = int(target_ratio)  # target_ratio debe ser entero\n",
    "        for cls, cnt in counts.items():\n",
    "            target = max(cnt, min_samples)\n",
    "            proportion_dict[cls] = target\n",
    "    else:\n",
    "        raise ValueError(\"strategy debe ser 'auto' o 'fixed_min'\")\n",
    "\n",
    "    return proportion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipelineSearch:\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                 class_names=None,\n",
    "                 imputation_methods=['bayesian'], #Se emplea unicamente bayesian regressor por justificacion tecnica anteriormente mostrada con base en distribuciones\n",
    "                 oversampling_methods=['none','adasyn'], #Se emplea unicamente adasyn y none por justificacion tecnica anteriormente mostrada con base en distribuciones\n",
    "                 oversampling_proportion_strategy='auto',\n",
    "                 oversampling_target_ratio=0.2,\n",
    "                 feature_selection_methods=[\n",
    "                     ('none', None),\n",
    "                     ('mutual_info', 50)\n",
    "                 ],\n",
    "                 classifiers=['rf', 'svm', 'xgb'],\n",
    "                 class_weight_methods=['balanced', 'sqrt_inv', 'log_inv'], #Seleccionar solo dos con base en colas de distribucion\n",
    "                 log_file='pipeline_search.log'):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.class_names = class_names if class_names is not None else [str(c) for c in np.unique(y_train)]\n",
    "        \n",
    "        self.imputation_methods = imputation_methods\n",
    "        self.oversampling_methods = oversampling_methods\n",
    "        self.oversampling_proportion_strategy = oversampling_proportion_strategy\n",
    "        self.oversampling_target_ratio = oversampling_target_ratio\n",
    "        self.feature_selection_methods = feature_selection_methods\n",
    "        self.classifiers = classifiers\n",
    "        self.class_weight_methods = class_weight_methods\n",
    "        self.log_file = log_file\n",
    "\n",
    "        self.results = [] #Aqui guardo resultados de cada combinacion para los tests\n",
    "        self.best_score = -1\n",
    "        self.best_config = None\n",
    "        self.best_model = None\n",
    "\n",
    "    def _get_class_weight_dict(self, method, y):\n",
    "        if method == 'balanced':\n",
    "            return balanced_class_weight(y)\n",
    "        elif method == 'sqrt_inv':\n",
    "            return sqrt_inverse_class_weight(y)\n",
    "        elif method == 'log_inv':\n",
    "            return log_inverse_class_weight(y)\n",
    "\n",
    "    def _get_hyperparameter_grid(self, classifier):\n",
    "        if classifier == 'rf':\n",
    "            return {\n",
    "                'n_estimators': [200, 400],\n",
    "                'max_depth': [None, 20],\n",
    "                'min_samples_leaf': [1, 4]\n",
    "            }\n",
    "        elif classifier == 'svm':\n",
    "            return {\n",
    "                'C': [1, 10, 100],\n",
    "                'gamma': ['scale', 0.01],\n",
    "                'kernel': ['rbf']\n",
    "            }\n",
    "        elif classifier == 'xgb':\n",
    "            return {\n",
    "                'n_estimators': [300, 600],\n",
    "                'max_depth': [4, 6],\n",
    "                'learning_rate': [0.05, 0.1]\n",
    "            }\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def _create_model(self, classifier, params, class_weight_dict):\n",
    "        if classifier == 'rf':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            return RandomForestClassifier(\n",
    "                class_weight=class_weight_dict,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                **params\n",
    "            )\n",
    "        elif classifier == 'svm':\n",
    "            from sklearn.svm import SVC\n",
    "            return SVC(\n",
    "                class_weight=class_weight_dict,\n",
    "                probability=True,\n",
    "                random_state=42,\n",
    "                **params\n",
    "            )\n",
    "        elif classifier == 'xgb':\n",
    "            import xgboost as xgb\n",
    "            return xgb.XGBClassifier(\n",
    "                eval_metric='mlogloss',\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                **params\n",
    "            )\n",
    "\n",
    "    def run(self):\n",
    "        # Calcular proporción de oversampling una vez\n",
    "        proportion_dict = get_oversampling_proportion(\n",
    "            self.y_train,\n",
    "            strategy=self.oversampling_proportion_strategy,\n",
    "            target_ratio=self.oversampling_target_ratio\n",
    "        )\n",
    "        print(f\"Proporción de oversampling utilizada: {proportion_dict}\\n\")\n",
    "\n",
    "        # Abrir archivo de log\n",
    "        with open(self.log_file, 'w') as log:\n",
    "            log.write(\"Pipeline Log\\n\")\n",
    "            log.write(f\"Proporción oversampling: {proportion_dict}\\n\\n\")\n",
    "\n",
    "        total_combinations = (len(self.imputation_methods) *\n",
    "                              len(self.oversampling_methods) *\n",
    "                              len(self.feature_selection_methods) *\n",
    "                              len(self.classifiers) *\n",
    "                              len(self.class_weight_methods))\n",
    "        print(f\"Total de combinaciones (sin contar hiperparámetros): {total_combinations}\")\n",
    "        print(\"Comenzando búsqueda...\\n\")\n",
    "\n",
    "        # Iterar sobre todas las combinaciones\n",
    "        for imp_method in self.imputation_methods:\n",
    "            # Imputación y escalado (se hace una vez por método de imputación)\n",
    "            X_train_scaled, X_val_scaled, X_test_scaled = escalado(self.X_train, self.X_val, self.X_test)\n",
    "\n",
    "            X_train_imp, X_val_imp, X_test_imp = imputacion_por_clase(\n",
    "                X_train_scaled, X_val_scaled, X_test_scaled, self.y_train, method=imp_method\n",
    "            )\n",
    "\n",
    "            for over_method in self.oversampling_methods:\n",
    "                # Oversampling solo sobre train\n",
    "                oversampler = OversamplingWithArtifactAnalysis(\n",
    "                    method=over_method,\n",
    "                    proportion=proportion_dict,\n",
    "                    random_state=42,\n",
    "                    n_neighbors=5\n",
    "                )\n",
    "                if over_method != 'none':\n",
    "                    X_train_over, y_train_over = oversampler.fit_resample(X_train_imp, self.y_train)\n",
    "                    oversampler.save_report(f\"artifacts_{imp_method}_{over_method}.json\")\n",
    "                else:\n",
    "                    X_train_over, y_train_over = X_train_imp, self.y_train\n",
    "\n",
    "                for fs_method, fs_k in self.feature_selection_methods:\n",
    "                    # Selección de características\n",
    "                    if fs_method == 'none':\n",
    "                        selector = FeatureSelector(method='none')\n",
    "                    else:\n",
    "                        selector = FeatureSelector(method=fs_method, k=fs_k, random_state=42)\n",
    "                    \n",
    "                    selector.fit(X_train_over, y_train_over)\n",
    "                    X_train_fs = selector.transform(X_train_over)\n",
    "                    X_val_fs = selector.transform(X_val_imp)\n",
    "                    X_test_fs = selector.transform(X_test_imp)\n",
    "\n",
    "                    for clf_name in self.classifiers:\n",
    "                        grid = self._get_hyperparameter_grid(clf_name)\n",
    "                        keys = list(grid.keys())\n",
    "                        values = list(grid.values())\n",
    "\n",
    "                        for weight_method in self.class_weight_methods:\n",
    "                            class_weight_dict = self._get_class_weight_dict(weight_method, self.y_train)\n",
    "\n",
    "                            for combination in itertools.product(*values):\n",
    "                                params = dict(zip(keys, combination))\n",
    "\n",
    "                                # Crear modelo\n",
    "                                model = self._create_model(clf_name, params, class_weight_dict)\n",
    "\n",
    "                                if clf_name == 'xgb':\n",
    "                                    # Calcular sample_weight a partir del diccionario de pesos\n",
    "                                    sample_weight = np.array([class_weight_dict[y] for y in y_train_over])\n",
    "                                    model.fit(\n",
    "                                        X_train_fs, y_train_over,\n",
    "                                        sample_weight=sample_weight,\n",
    "                                        eval_set=[(X_val_fs, self.y_val)],\n",
    "                                        verbose=False\n",
    "                                    )\n",
    "                                else:\n",
    "                                    model.fit(X_train_fs, y_train_over)\n",
    "\n",
    "                                # Evaluar en validación\n",
    "                                y_val_pred = model.predict(X_val_fs)\n",
    "                                f1_macro = f1_score(self.y_val, y_val_pred, average='macro')\n",
    "                                recall_per_class = recall_score(self.y_val, y_val_pred, average=None)\n",
    "\n",
    "                                # Guardar resultado\n",
    "                                result = {\n",
    "                                    'imputation': imp_method,\n",
    "                                    'oversampling': over_method,\n",
    "                                    'feature_selection': f\"{fs_method}_{fs_k}\" if fs_method != 'none' else 'none',\n",
    "                                    'classifier': clf_name,\n",
    "                                    'class_weight': weight_method,\n",
    "                                    'params': params,\n",
    "                                    'f1_macro_val': f1_macro,\n",
    "                                    'recall_val': recall_per_class.tolist()\n",
    "                                }\n",
    "                                self.results.append(result)\n",
    "\n",
    "                                # Actualizar mejor modelo\n",
    "                                if f1_macro > self.best_score:\n",
    "                                    self.best_score = f1_macro\n",
    "                                    self.best_config = result.copy()\n",
    "                                    self.best_model = model\n",
    "                                    # Guardar también los conjuntos transformados para test (necesarios para evaluar después)\n",
    "                                    self.best_X_test_fs = X_test_fs\n",
    "                                    self.best_y_test = self.y_test\n",
    "                                    self.best_class_names = self.class_names\n",
    "\n",
    "                                # Log inmediato\n",
    "                                log_line = (f\"imp={imp_method}, over={over_method}, fs={fs_method}_{fs_k}, \"\n",
    "                                            f\"clf={clf_name}, weight={weight_method}, params={params}, \"\n",
    "                                            f\"f1_macro_val={f1_macro:.4f}\\n\")\n",
    "                                print(log_line.strip())\n",
    "                                with open(self.log_file, 'a') as log:\n",
    "                                    log.write(log_line)\n",
    "\n",
    "        print(\"\\nBusqueda finalizada\")\n",
    "        print(f\"Mejor F1 macro en validación: {self.best_score:.4f}\")\n",
    "        print(\"Mejor configuración:\", self.best_config)\n",
    "\n",
    "    def evaluate_best_on_test(self):\n",
    "        print(\"\\nEvaluacion en test\")\n",
    "        y_test_pred = self.best_model.predict(self.best_X_test_fs)\n",
    "\n",
    "        # Métricas\n",
    "        f1_macro = f1_score(self.best_y_test, y_test_pred, average='macro')\n",
    "        accuracy = accuracy_score(self.best_y_test, y_test_pred)\n",
    "        recall_per_class = recall_score(self.best_y_test, y_test_pred, average=None)\n",
    "        conf_matrix = confusion_matrix(self.best_y_test, y_test_pred)\n",
    "        class_report = classification_report(self.best_y_test, y_test_pred,\n",
    "                                             target_names=self.best_class_names, digits=4)\n",
    "\n",
    "        # Mostrar en consola\n",
    "        print(\"Mejor configuración:\")\n",
    "        print(json.dumps(self.best_config, indent=2))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "        print(\"Recall por clase:\", recall_per_class)\n",
    "        print(\"Matriz de confusión:\\n\", conf_matrix)\n",
    "\n",
    "        # Guardar en archivo de log\n",
    "        with open(self.log_file, 'a') as log:\n",
    "            log.write(\"\\nEvaluacion en Test\\n\")\n",
    "            log.write(f\"Mejor configuración:\\n{json.dumps(self.best_config, indent=2)}\\n\")\n",
    "            log.write(\"\\nClassification Report:\\n\")\n",
    "            log.write(class_report)\n",
    "            log.write(f\"\\nAccuracy: {accuracy:.4f}\\n\")\n",
    "            log.write(f\"F1 Macro: {f1_macro:.4f}\\n\")\n",
    "            log.write(f\"Recall por clase: {recall_per_class}\\n\")\n",
    "            log.write(f\"Matriz de confusión:\\n{conf_matrix}\\n\")\n",
    "\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=self.best_class_names, yticklabels=self.best_class_names)\n",
    "        plt.title('Matriz de confusión en test')\n",
    "        plt.savefig('confusion_matrix_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = FullPipelineSearch(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    class_names=le.classes_,\n",
    "    imputation_methods=['bayesian'],\n",
    "    oversampling_methods=['none', 'adasyn'],\n",
    "    oversampling_proportion_strategy='auto',\n",
    "    oversampling_target_ratio=0.2,\n",
    "    feature_selection_methods=[\n",
    "        ('none', None),\n",
    "        ('mutual_info', 50)\n",
    "    ],\n",
    "    classifiers=['rf', 'svm', 'xgb'],\n",
    "    class_weight_methods=['balanced', 'sqrt_inv', 'log_inv'],\n",
    "    log_file='resultados_pipeline.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar búsqueda del mejor modelo\n",
    "search.run()\n",
    "# Evaluar el mejor modelo en test\n",
    "search.evaluate_best_on_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
