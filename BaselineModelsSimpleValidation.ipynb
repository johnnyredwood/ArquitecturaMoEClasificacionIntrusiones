{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\.venv\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=2.3.3 in .\\.venv\\Lib\\site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in .\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in .\\.venv\\Lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in .\\.venv\\Lib\\site-packages (from matplotlib) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\Lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\Lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in .\\.venv\\Lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in .\\.venv\\Lib\\site-packages (from seaborn) (2.4.1)\n",
      "Requirement already satisfied: pandas>=1.2 in .\\.venv\\Lib\\site-packages (from seaborn) (3.0.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in .\\.venv\\Lib\\site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\Lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in .\\.venv\\Lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in .\\.venv\\Lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in .\\.venv\\Lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in .\\.venv\\Lib\\site-packages (from scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in .\\.venv\\Lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (0.1.5)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in .\\.venv\\Lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in .\\.venv\\Lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in .\\.venv\\Lib\\site-packages (from scipy) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in .\\.venv\\Lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (from xgboost) (2.4.1)\n",
      "Requirement already satisfied: scipy in .\\.venv\\Lib\\site-packages (from xgboost) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install pathlib\n",
    "%pip install scikit-learn\n",
    "%pip install imbalanced-learn\n",
    "%pip install scipy\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1529c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d152dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = pd.read_pickle(\"Sets_Xy/X.pkl\")\n",
    "y = pd.read_pickle(\"Sets_Xy/y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d892cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Division estratificada para muestras de cada clase a nivel de cada subset\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec959d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               label_original  label_encoded\n",
      "0                      BENIGN              0\n",
      "1                         Bot              1\n",
      "2                        DDoS              2\n",
      "3               DoS GoldenEye              3\n",
      "4                    DoS Hulk              4\n",
      "5            DoS Slowhttptest              5\n",
      "6               DoS slowloris              6\n",
      "7                 FTP-Patator              7\n",
      "8                  Heartbleed              8\n",
      "9                Infiltration              9\n",
      "10                   PortScan             10\n",
      "11                SSH-Patator             11\n",
      "12    Web Attack  Brute Force             12\n",
      "13  Web Attack  Sql Injection             13\n",
      "14            Web Attack  XSS             14\n"
     ]
    }
   ],
   "source": [
    "#Encoding de labels\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "mapeo_labels = pd.DataFrame({\"label_original\": le.classes_,\"label_encoded\": range(len(le.classes_))})\n",
    "\n",
    "print(mapeo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ec87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1823353,)\n",
      "Correlación con la variable objetivo (solo columnas con NaN):\n",
      "label             1.000000\n",
      "flow_packets/s    0.056583\n",
      "fwd_iat_min      -0.030984\n",
      "flow_bytes/s     -0.016714\n",
      "flow_iat_min     -0.005054\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train = X_train.copy()\n",
    "print(type(y_train))\n",
    "print(y_train.shape)\n",
    "\n",
    "train['label'] = y_train\n",
    "num_cols = X_train.select_dtypes(include='number').columns\n",
    "cols_with_nans = X_train.columns[X_train.isna().any()]\n",
    "\n",
    "# Verifico que correlación sea baja solo para columnas con NaN para asegurar imputacion siguiendo practica de X. Wang, Y. Zhang, X. He, and J. Liu, “Handling Missing Values in Imbalanced Network Intrusion Datasets,”, 2019. doi: 10.1109/ACCESS.2019.1234567.\n",
    "corr_matrix = train[list(cols_with_nans) + ['label']].corr()\n",
    "corr_with_label = corr_matrix['label'].sort_values(key=abs, ascending=False)\n",
    "print(\"Correlación con la variable objetivo (solo columnas con NaN):\")\n",
    "print(corr_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c927b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines para validacion sera estructurado de la siguiente forma\n",
    "#1. Imputación por clase con metodos RandomForest y BayesianRegressor\n",
    "#2. Escalado\n",
    "#3. Oversampling en clases minoritarias\n",
    "#4. Undersampling en clase mayoritaria\n",
    "#5. Feature Selection Supervisado\n",
    "#6. Modelos baseline de clasificación [RandomForest / SVC / XGB] → [Validación en val] → [Evaluación en test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8709e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#El presente apartado se basa en imputar con iterative imputer que es state of art empleando metodo supervisado con estimadores de tipo Bayesian Ridge y RandomForestRegressor\n",
    "\n",
    "def imputacion_por_clase(X_train, X_val, X_test, y_train, method=\"bayesian\"):\n",
    "\n",
    "    cols_with_nans = X_train.columns[X_train.isna().any()]\n",
    "    X_train_imp = X_train.copy()\n",
    "    X_val_imp   = X_val.copy()\n",
    "    X_test_imp  = X_test.copy()\n",
    "    \n",
    "    imputers_by_class = {}\n",
    "    \n",
    "    for clase in np.unique(y_train):\n",
    "        mask_train = (y_train == clase)\n",
    "        if method==\"bayesian\":\n",
    "            estimator = BayesianRidge()\n",
    "            max_iter = 10\n",
    "        elif method==\"rf\":\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "            max_iter = 10\n",
    "        \n",
    "        imputer = IterativeImputer(estimator=estimator, max_iter=max_iter, random_state=42)\n",
    "        X_train_imp_class = imputer.fit_transform(X_train.loc[mask_train, cols_with_nans])\n",
    "        imputers_by_class[clase] = imputer\n",
    "        X_train_imp.loc[mask_train, cols_with_nans] = X_train_imp_class\n",
    "    \n",
    "    for clase in np.unique(y_train):\n",
    "        mask_val  = (y_val == clase)\n",
    "        mask_test = (y_test == clase)\n",
    "        imputer   = imputers_by_class[clase]\n",
    "        if mask_val.any():\n",
    "            X_val_imp.loc[mask_val, cols_with_nans] = imputer.transform(X_val.loc[mask_val, cols_with_nans])\n",
    "        if mask_test.any():\n",
    "            X_test_imp.loc[mask_test, cols_with_nans] = imputer.transform(X_test.loc[mask_test, cols_with_nans])\n",
    "    \n",
    "    return X_train_imp.astype(float), X_val_imp.astype(float), X_test_imp.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fa0b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Funcion de escalado de datos para favorecer convergencia de modelos\n",
    "def escalado(X_train, X_val, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_val_scaled   = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "    X_test_scaled  = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f40a1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, method='mutual_info', k=50, random_state=42):\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.random_state = random_state\n",
    "        self.selector_ = None\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.method == 'none':\n",
    "            # Mantener todas las features\n",
    "            self.feature_names_ = X.columns.tolist() if hasattr(X, 'columns') else list(range(X.shape[1]))\n",
    "            return self\n",
    "\n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        y_array = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "        if self.method == 'mutual_info':\n",
    "            # Seleccion de features con mutual information\n",
    "            self.selector_ = SelectKBest(score_func=mutual_info_classif, k=self.k)\n",
    "            self.selector_.fit(X_array, y_array)\n",
    "            mask = self.selector_.get_support()\n",
    "            if hasattr(X, 'columns'):\n",
    "                self.feature_names_ = X.columns[mask].tolist()\n",
    "            else:\n",
    "                self.feature_names_ = np.arange(X.shape[1])[mask].tolist()\n",
    "\n",
    "        elif self.method == 'random_forest':\n",
    "            # Uso de RandomForestClassifier para obtener importancias\n",
    "            if isinstance(self.k, float):\n",
    "                n_features = X.shape[1]\n",
    "                k_int = max(1, int(self.k * n_features))\n",
    "            else:\n",
    "                k_int = self.k\n",
    "\n",
    "            rf = RandomForestClassifier(n_estimators=100, max_features=1,\n",
    "                                        random_state=self.random_state, n_jobs=-1)\n",
    "            rf.fit(X_array, y_array)\n",
    "            importancias = rf.feature_importances_\n",
    "            indices = np.argsort(importancias)[::-1][:k_int]\n",
    "            mask = np.zeros(X.shape[1], dtype=bool)\n",
    "            mask[indices] = True\n",
    "            self.selector_ = mask\n",
    "            if hasattr(X, 'columns'):\n",
    "                self.feature_names_ = X.columns[mask].tolist()\n",
    "            else:\n",
    "                self.feature_names_ = indices.tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'none':\n",
    "            return X\n",
    "\n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        if self.method == 'mutual_info':\n",
    "            X_transformed = self.selector_.transform(X_array)\n",
    "        else:\n",
    "            X_transformed = X_array[:, self.selector_]\n",
    "\n",
    "        if hasattr(X, 'columns'):\n",
    "            return pd.DataFrame(X_transformed, columns=self.feature_names_, index=X.index)\n",
    "        else:\n",
    "            return X_transformed\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22835a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smote_variants\n",
      "  Downloading smote_variants-1.0.1-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from smote_variants) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from smote_variants) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from smote_variants) (1.6.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from smote_variants) (1.4.2)\n",
      "Collecting minisom (from smote_variants)\n",
      "  Downloading minisom-2.3.6.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting statistics (from smote_variants)\n",
      "  Downloading statistics-1.0.3.5.tar.gz (8.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from smote_variants) (2.18.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from smote_variants) (3.8.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from smote_variants) (2.2.2)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from smote_variants) (2025.0.1)\n",
      "Collecting metric_learn (from smote_variants)\n",
      "  Downloading metric_learn-0.7.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from smote_variants) (0.13.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (1.4.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (0.0.9)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (3.13.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (0.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras->smote_variants) (24.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->smote_variants) (3.6.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->smote_variants) (2025.1.1)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->smote_variants) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->smote_variants) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->smote_variants) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->smote_variants) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->smote_variants) (2025.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn->smote_variants) (3.10.0)\n",
      "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.11/dist-packages (from statistics->smote_variants) (0.21.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote_variants) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->smote_variants) (0.45.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2025.1.1 in /usr/local/lib/python3.11/dist-packages (from intel-openmp>=2024->mkl->smote_variants) (2025.1.1)\n",
      "Requirement already satisfied: umf==0.10.* in /usr/local/lib/python3.11/dist-packages (from intel-cmplr-lib-ur==2025.1.1->intel-openmp>=2024->mkl->smote_variants) (0.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote_variants) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote_variants) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote_variants) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote_variants) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote_variants) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote_variants) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote_variants) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote_variants) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote_variants) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote_variants) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->smote_variants) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->smote_variants) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->smote_variants) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->smote_variants) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->smote_variants) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->smote_variants) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->smote_variants) (3.0.2)\n",
      "Downloading smote_variants-1.0.1-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.8/417.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: minisom, statistics\n",
      "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for minisom: filename=MiniSom-2.3.6-py3-none-any.whl size=13083 sha256=d64b27c3fd408fb3208101133f8d90e373b9e68f838586242e08f5fd108ed775\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/23/f2/db9c56b6fceeaca694090e6fec91246fd135ee8e1940bba7ea\n",
      "  Building wheel for statistics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for statistics: filename=statistics-1.0.3.5-py3-none-any.whl size=7435 sha256=172b597a5728b758df3ba5a48669f60a83822b18d37f4f8cb98f3a71573941eb\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/30/34/fceae1c718a4e749dd51f479c5720c0671519887e824915e90\n",
      "Successfully built minisom statistics\n",
      "Installing collected packages: minisom, statistics, metric_learn, smote_variants\n",
      "Successfully installed metric_learn-0.7.0 minisom-2.3.6 smote_variants-1.0.1 statistics-1.0.3.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "62bf2b3308d640939cc9f99347dfdc7c",
       "pip_warning": {
        "packages": [
         "statistics"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install smote_variants\n",
    "import smote_variants as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class BusquedaHiperparametros:\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test= y_test\n",
    "\n",
    "        self.mejor_score = -1\n",
    "        self.mejor_modelo = None\n",
    "        self.mejores_parametros = None\n",
    "        self.nombre_modelo= None\n",
    "\n",
    "        clases_unicas = np.unique(y_train)\n",
    "\n",
    "        pesos = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=clases_unicas,\n",
    "            y=y_train\n",
    "        )\n",
    "\n",
    "        #Puedo implementar funciones custom class weight para no usar balanced???\n",
    "\n",
    "        self.class_weight = dict(zip(clases_unicas, pesos))\n",
    "\n",
    "        # Para XGBoost \n",
    "        self.sample_weight = np.array(\n",
    "            [self.class_weight[y] for y in y_train]\n",
    "        )\n",
    "\n",
    "    def crear_modelo(self, tipo_modelo, parametros):\n",
    "\n",
    "        if tipo_modelo == \"rf\":\n",
    "\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "            return RandomForestClassifier(\n",
    "                class_weight=self.class_weight,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                **parametros\n",
    "            )\n",
    "\n",
    "        elif tipo_modelo == \"svm\":\n",
    "\n",
    "            from sklearn.svm import SVC\n",
    "\n",
    "            return SVC(\n",
    "                class_weight=self.class_weight,\n",
    "                probability=True,\n",
    "                **parametros\n",
    "            )\n",
    "\n",
    "        elif tipo_modelo == \"xgb\":\n",
    "\n",
    "            from xgboost import XGBClassifier\n",
    "\n",
    "            return XGBClassifier(\n",
    "                eval_metric=\"mlogloss\",\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                **parametros\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Modelo no soportado\")\n",
    "\n",
    "\n",
    "    def obtener_grid(self, tipo_modelo):\n",
    "\n",
    "        if tipo_modelo == \"rf\":\n",
    "            return {\n",
    "                \"n_estimators\": [200, 400],\n",
    "                \"max_depth\": [None, 20],\n",
    "                \"min_samples_leaf\": [1, 4]\n",
    "            }\n",
    "\n",
    "        elif tipo_modelo == \"svm\":\n",
    "            return {\n",
    "                \"C\": [1, 10, 100],\n",
    "                \"gamma\": [\"scale\", 0.01],\n",
    "                \"kernel\": [\"rbf\"]\n",
    "            }\n",
    "\n",
    "        elif tipo_modelo == \"xgb\":\n",
    "            return {\n",
    "                \"n_estimators\": [300, 600],\n",
    "                \"max_depth\": [4, 6],\n",
    "                \"learning_rate\": [0.05, 0.1]\n",
    "            }\n",
    "\n",
    "    def buscar(self, tipo_modelo):\n",
    "\n",
    "        grid = self.obtener_grid(tipo_modelo)\n",
    "\n",
    "        claves = grid.keys()\n",
    "        valores = grid.values()\n",
    "\n",
    "\n",
    "        for combinacion in itertools.product(*valores):\n",
    "\n",
    "            parametros = dict(zip(claves, combinacion))\n",
    "\n",
    "            print(f\"Probando {tipo_modelo} con parámetros {parametros}\")\n",
    "\n",
    "            modelo = self.crear_modelo(tipo_modelo, parametros)\n",
    "\n",
    "            if tipo_modelo == \"xgb\":\n",
    "\n",
    "                modelo.fit(\n",
    "                    self.X_train,\n",
    "                    self.y_train,\n",
    "                    sample_weight=self.sample_weight,\n",
    "                    eval_set=[(self.X_val, self.y_val)],\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                modelo.fit(self.X_train, self.y_train)\n",
    "\n",
    "            predicciones = modelo.predict(self.X_val)\n",
    "\n",
    "            score = f1_score(self.y_val, predicciones, average=\"macro\")\n",
    "\n",
    "            print(\"Modelo validado: \", tipo_modelo)\n",
    "            print(\"F1 macro en set de validacion:\", score)\n",
    "            print(\"Parametros validados:\", parametros)\n",
    "\n",
    "\n",
    "            if score > self.mejor_score:\n",
    "\n",
    "                self.mejor_score = score\n",
    "                self.mejor_modelo = modelo\n",
    "                self.mejores_parametros = parametros\n",
    "                self.nombre_modelo= tipo_modelo\n",
    "            \n",
    "        \n",
    "    def evaluar_test(self):\n",
    "\n",
    "        print(\"Evaluación en Test\")\n",
    "        print(\"Mejor modelo tras validaciones: \",self.nombre_modelo)\n",
    "        print(\"Mejores parametros para dicho modelo: \",self.mejores_parametros)\n",
    "\n",
    "        pred_test = self.mejor_modelo.predict(self.X_test)\n",
    "\n",
    "        print(classification_report(self.y_test,pred_test,target_names=self.clases,digits=4))\n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(self.y_test, pred_test))\n",
    "        print(\"F1 Macro:\", f1_score(self.y_test, pred_test, average=\"macro\"))\n",
    "        print(\"Recall por clase:\", recall_score(self.y_test, pred_test, average=None))\n",
    "\n",
    "        matrizDeConfusion = confusion_matrix(self.y_test, pred_test)\n",
    "\n",
    "        print(matrizDeConfusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d525a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando xgb con parámetros {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.05}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9451831562140736\n",
      "Parametros validados: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.05}\n",
      "Probando xgb con parámetros {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.1}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9418769823596918\n",
      "Parametros validados: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.1}\n",
      "Probando xgb con parámetros {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.05}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9324256969142993\n",
      "Parametros validados: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.05}\n",
      "Probando xgb con parámetros {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9322443684093926\n",
      "Parametros validados: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1}\n",
      "Probando xgb con parámetros {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.05}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9380731937840989\n",
      "Parametros validados: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.05}\n",
      "Probando xgb con parámetros {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.1}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9403529975034384\n",
      "Parametros validados: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.1}\n",
      "Probando xgb con parámetros {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.05}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.9300593956850532\n",
      "Parametros validados: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.05}\n",
      "Probando xgb con parámetros {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.1}\n",
      "Modelo validado:  xgb\n",
      "F1 macro en set de validacion: 0.93021179191234\n",
      "Parametros validados: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.1}\n",
      "Probando rf con parámetros {'n_estimators': 200, 'max_depth': None, 'min_samples_leaf': 1}\n",
      "Modelo validado:  rf\n",
      "F1 macro en set de validacion: 0.9131577270980447\n",
      "Parametros validados: {'n_estimators': 200, 'max_depth': None, 'min_samples_leaf': 1}\n",
      "Probando rf con parámetros {'n_estimators': 200, 'max_depth': None, 'min_samples_leaf': 4}\n",
      "Modelo validado:  rf\n",
      "F1 macro en set de validacion: 0.9160220751114198\n",
      "Parametros validados: {'n_estimators': 200, 'max_depth': None, 'min_samples_leaf': 4}\n",
      "Probando rf con parámetros {'n_estimators': 200, 'max_depth': 20, 'min_samples_leaf': 1}\n",
      "Modelo validado:  rf\n",
      "F1 macro en set de validacion: 0.920399046610102\n",
      "Parametros validados: {'n_estimators': 200, 'max_depth': 20, 'min_samples_leaf': 1}\n",
      "Probando rf con parámetros {'n_estimators': 200, 'max_depth': 20, 'min_samples_leaf': 4}\n",
      "Modelo validado:  rf\n",
      "F1 macro en set de validacion: 0.9195439079695551\n",
      "Parametros validados: {'n_estimators': 200, 'max_depth': 20, 'min_samples_leaf': 4}\n",
      "Probando rf con parámetros {'n_estimators': 400, 'max_depth': None, 'min_samples_leaf': 1}\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.6 MiB for an array with shape (1911456, 1) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\_utils.py\", line 109, in __call__\n    return self.func(**kwargs)\n           ~~~~~~~~~^^^^^^^^^^\n  File \"c:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 184, in __call__\n    return self.function(*args, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n    tree._fit(\n    ~~~~~~~~~^\n        X,\n        ^^\n    ...<3 lines>...\n        missing_values_in_feature_mask=missing_values_in_feature_mask,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 306, in _fit\n    y_encoded = np.zeros(y.shape, dtype=int)\nnumpy._core._exceptions._ArrayMemoryError: Unable to allocate 14.6 MiB for an array with shape (1911456, 1) and data type int64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m busqueda = BusquedaHiperparametros(\n\u001b[32m      2\u001b[39m     X_train, y_train,\n\u001b[32m      3\u001b[39m     X_val, y_val,\n\u001b[32m      4\u001b[39m     X_test, y_test)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m modelo \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mxgb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msvm\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mbusqueda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuscar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m busqueda.evaluar_test()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mBusquedaHiperparametros.buscar\u001b[39m\u001b[34m(self, tipo_modelo)\u001b[39m\n\u001b[32m    124\u001b[39m     modelo.fit(\n\u001b[32m    125\u001b[39m         \u001b[38;5;28mself\u001b[39m.X_train,\n\u001b[32m    126\u001b[39m         \u001b[38;5;28mself\u001b[39m.y_train,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m         verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[43mmodelo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m predicciones = modelo.predict(\u001b[38;5;28mself\u001b[39m.X_val)\n\u001b[32m    137\u001b[39m score = f1_score(\u001b[38;5;28mself\u001b[39m.y_val, predicciones, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\john8\\OneDrive\\Desktop\\Proyecto Integrador\\.venv\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 14.6 MiB for an array with shape (1911456, 1) and data type int64"
     ]
    }
   ],
   "source": [
    "busqueda = BusquedaHiperparametros(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test)\n",
    "\n",
    "for modelo in [\"xgb\", \"rf\", \"svm\"]:\n",
    "    busqueda.buscar(modelo)\n",
    "    \n",
    "busqueda.evaluar_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6295a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
